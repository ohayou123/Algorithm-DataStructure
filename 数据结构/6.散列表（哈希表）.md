# 目录
[toc]

## 介绍
散列表（Hash table，也叫哈希表），是根据键（Key）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。

散列函数，顾名思义，它是一个函数。如果把它定义成 hash(key) ，其中 key 表示元素的键值，则 hash(key) 的值表示经过散列函数计算得到的散列值。

### 设计思路
散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515211747.png)

>例子:
为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。号码用6位数字来表示。比如051167，其中，前两位05表示年级，中间两位11表示班级，最后两位还是原来的编号1到89。这个时候我们该如何存储选手信息，才能够支持通过编号来快速查找选手信息呢？
- 我们可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。
- 因为参赛编号跟数组下标一一对应，当我们需要查询参赛编号为x的选手的时候，我们只需要将下标为x的数组元素取出来就可以了，时间复杂度就是O(1)。这样按照编号查找选手信息.
- 这就是典型的散列思想。其中，参赛选手的编号我们叫作键（key）或者关键字。我们用它来标识一个选手。我们把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash值”“哈希值”）。


## 散列函数

散列函数，顾名思义，它是一个函数。我们可以把它定义成hash(key)，其中key表示元素的键值，hash(key)的值表示经过散列函数计算得到的散列值。++哈希表适合插入和删除比较少（尽量少的扩容和缩容）。++

### 散列函数的特点：

1. 确定性：如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。

2. 散列碰撞（collision）：散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同。

3. 不可逆性：一个哈希值对应无数个明文，理论上你并不知道哪个是。


4. 混淆特性：
输入一些数据计算出散列值，然后部分改变输入值，一个具有强混淆特性的散列函数会产生一个完全不同的散列值。



例子中，编号就是数组下标，所以hash(key)就等于key。改造后的例子，写成散列函数稍微有点复杂。我用伪代码将它写成函数就是下面这样：

```
int hash(String key) {
  // 获取后两位字符
  string lastTwoChars = key.substr(length-2, length);
  // 将后两位字符转换为整数
  int hashValue = convert lastTwoChas to int-type;
  return hashValue;
}
```
### 常见的散列函数
1. MD5

MD5 即 Message-Digest Algorithm 5（信息-摘要算法5），用于确保信息传输完整一致。是计算机广泛使用的杂凑算法之一，主流编程语言普遍已有 MD5 实现。

将数据（如汉字）运算为另一固定长度值，是杂凑算法的基础原理，MD5 的前身有 MD2 、MD3 和 MD4 。

MD5 是输入不定长度信息，输出固定长度 128-bits 的算法。经过程序流程，生成四个32位数据，最后联合起来成为一个 128-bits 散列。

基本方式为，求余、取余、调整长度、与链接变量进行循环运算，得出结果。

MD5 计算广泛应用于错误检查。在一些 BitTorrent 下载中，软件通过计算 MD5 来检验下载到的碎片的完整性。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/MD5072546.gif)


2. SHA-1

SHA-1（英语：Secure Hash Algorithm 1，中文名：安全散列算法1）是一种密码散列函数，SHA-1可以生成一个被称为消息摘要的160位（20字节）散列值，散列值通常的呈现形式为40个十六进制数。

SHA-1 曾经在许多安全协议中广为使用，包括TLS和SSL、PGP、SSH、S/MIME和IPsec，曾被视为是MD5的后继者。




### 该如何构造散列函数呢？

1. 散列函数计算得到的散列值是一个非负整数；
2. 如果key1 = key2，那hash(key1) == hash(key2)；
3. 如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。

### 常见的哈希函数的构造思路有六种：
#### 1. 直接定值法
>直接定制法的哈希函数是一个一次函数，取关键字和关机字的某个线性函数值为哈希地址，有如下两种形式：
- y = a;
- y = na+mb;

#### 2. 数字分析法
>如果关键字由多位字符或者数字组成，就可以考虑抽取其中的 2 位或者多位作为该关键字对应的哈希地址，在取法上尽量选择变化较多的位，避免冲突发生。
![image](8D1243D3D53B4FD5B643B5DEF0880D60)

#### 3. 平方取中法（较常用的一种）
>平方取中法是对关键字做平方操作，取中间几位作为哈希地址(此方法是比较常用的构造哈希函数的方法) 
例如关键字序列为{421，423，436}，对各个关键字进行平方后的结果为{177241，178929，190096}，我们则可以取中间的两位{72，89，00}作为其哈希地址。
![image](D64827037D4D49AFBAD60596C1C538CD)
#### 4. 折叠法
>是将关键字分割成位数相同的几部分（最后一部分的位数可以不同），然后取这几部分的叠加和（舍去进位）作为哈希地址。关键字位数很多，且关键字每一位上数字分布大致均匀时，可以采用折叠法。折叠又可以分为两种：
- 移位折叠 :(移动折叠是将分割后的每一部分的最低位对齐，然后想加)
- 间界折叠 :(间接叠加是从一端向另一端来回折叠，然后相加)
![image](1BEC2767BE4B447C844D8F3DA2BD3FDF)

#### 5. 除留余数法
>若已知整个哈希表的最大长度 m，可以取一个不大于 m 的数 p，然后对该关键字 key 做取余运算.

```
即: f（key） = key % p
```


#### 6. 随机数法
>随机数法既是取关键字的一个随机函数值作为它的哈希地址，适用于关键字长度不一的情况，如: f (key) = random (key)或者：

```
（1）计算哈希函数所需时间（包括硬件指令的因素）
（2）关键字的长度
（3）哈希表的大小
（4）关键字的分布情况
（5）记录的查找频率
```



## 散列冲突
再好的散列函数也无法避免散列冲突。
>这涉及到数学中比较好理解的一个原理：抽屉原理。抽屉原理：桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面至少放两个苹果。这一现象就是我们所说的“抽屉原理”。

那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。

### 1. 开放寻址法
开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。

1. 优点：这种方法实现的散列表，序列化起来比较简单
2. 缺点：用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。
3. 适用场景：当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。


开放寻址法是一种解决碰撞的方法，对于开放寻址冲突解决方法，比较经典的有线性探测方法（Linear Probing）、二次探测（Quadratic probing）和 双重散列（Double hashing）等方法
#### 比如线性探测（Linear Probing）。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/XIANXINGTANCEC71670E.gif)
1. 当我们往散列表中==插入数据==时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515211849.png)
- 从图中可以看出，散列表的大小为10，在元素x插入散列表之前，已经6个元素插入到散列表中。x经过Hash算法之后，被散列到位置下标为7的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置2，于是将其插入到这个位置。


2. 在散列表中==查找元素==的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515211906.png)

3. 对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。这是为什么呢？

    我们可以将删除的元素，==特殊标记为deleted==。当线性探测查找的时候，遇到标记为deleted的空间，并不是停下来，而是继续往下探测。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515211919.png)

>线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。

对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，二次探测（Quadratic probing）和双重散列（Double hashing）。

#### 二次探测（Quadratic probing）和双重散列（Double hashing）。

1. 所谓二次探测，跟线性探测很像，线性探测每次探测的步长是1，那它探测的下标序列就是hash(key)+0，hash(key)+1，hash(key)+2……==而二次探测探测的步长就变成了原来的“二次方”==，也就是说，它探测的下标序列就是hash(key)+0，hash(key)+^12，hash(key)+2^2。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/ERCITANCEC771.gif)
2. 所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数hash1(key)，hash2(key)，hash3(key)……我们==先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推==，直到找到空闲的存储位置。  
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/SHUANGCHONGTANCE.gif)


#### 装载因子
加载因子是表示 Hsah 表中元素的填满的程度，若加载因子越大，则填满的元素越多,这样的好处是：空间利用率高了,但冲突的机会加大了。反之,加载因子越小,填满的元素越少,好处是冲突的机会减小了，但空间浪费多了。
>不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，==我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少==。

```
装载因子的计算公式是：
散列表的装载因子=填入表中的元素个数/散列表的长度
```
装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。
##### 装载因子过大怎么办？
- 针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了0.4。

- 针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515212048.png)

装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于1。

#### 如何避免低效地扩容？
- 为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。

- 当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515212115.png)

这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。

通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是O(1)。

### 2. 链表法
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/lianbiaofa227.gif)
链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。我们来看这个图，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515211949.png)

- 当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是O(1)。

- 当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。这两个操作的时间复杂度跟链表的长度k成正比，也就是O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中n表示散列中数据的个数，m表示散列表中“槽”的个数。

1. 优点：首先，链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。
2. 缺点：链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对CPU缓存是不友好的，这方面对于执行效率也有一定的影响。
>如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4个字节或者8个字节），那链表中指针的内存消耗在大对象面前就可以忽略了.
3. 优化：实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是O(logn)。这样也就有效避免了前面讲到的散列碰撞攻击。
![image](B863205A8E1E462F910FBBE02C91999D)
4.适用场景：基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。



## 解答

###  Word文档中单词拼写检查功能是如何实现的？
- 常用的英文单词有20万个左右，假设单词的平均长度是10个字母，平均一个单词占用10个字节的内存空间，那20万英文单词大约占2MB的存储空间，就算放大10倍也就是20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。
    
- 当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。


```
将单词中每个字母的ASCll码值“进位”相加，
然后再跟散列表的大小求余、取模，作为散列值。

比如，英文单词nice，我们转化出来的散列值就是下面这样：
hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978
```

### 假设我们有10万条URL访问日志，如何按照访问次数给URL排序？
- 遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 O(N)。
    
- 如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。


### 有两个字符串数组，每个数组大约有10万条字符串，如何快速找出两个数组中相同的字符串？

- 以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。
- 再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 O(N)。 
    
### 如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？

    首先，散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能。其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。

    >工业级散列表举例分析：Java中的HashMap这样一个工业级的散列表

    1. 初始大小
    HashMap默认的初始大小是16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高HashMap的性能。
     
    2. 装载因子和动态扩容
    最大装载因子默认是0.75，当HashMap中元素个数超过0.75*capacity（capacity表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。
    
    3. 散列冲突解决方法
    HashMap底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。
    
        于是，在JDK1.8版本中，为了对HashMap做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高HashMap的性能。当红黑树结点个数少于8个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。
    
    4. 散列函数
    散列函数的设计并不复杂，追求的是简单高效、分布均匀。
    
    ```
    int hash(Object key) {
        int h = key.hashCode()；
        return (h ^ (h >>> 16)) & (capitity -1); //capicity表示散列表的大小
    }
    
    public int hashCode() {
      int var1 = this.hash;
      if(var1 == 0 && this.value.length > 0) {
        char[] var2 = this.value;
        for(int var3 = 0; var3 < this.value.length; ++var3) {
          var1 = 31 * var1 + var2[var3];
        }
        this.hash = var1;
      }
      return var1;
    }
    
    ```

### LRU缓存淘汰算法
如果单纯地采用链表的话，时间复杂度只能是O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到O(1)。具体的结构就是下面这个样子：
![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515212200.png)
我们使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段hnext==。前驱和后继指针是为了将结点串在双向链表中，hnext指针是为了将结点串在散列表的拉链中。==

- 往缓存中添加一个数据：通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。

- 从缓存中删除一个数据：借助散列表，我们可以在O(1)时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针O(1)时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要O(1)的时间复杂度。

- 在缓存中查找一个数据：需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。

### Redis有序集合
    实际上，在有序集合中，每个成员对象有两个重要的属性，key（键值）和score（分值）。我们不仅会通过score来查找数据，还会通过key来查找数据。
    
        >举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的ID来查找积分信息，也可以通过积分区间来查找用户ID或者姓名信息。这里包含ID、姓名和积分的用户信息，就是成员对象，用户ID就是key，积分就是score。
    
    如果我们细化一下Redis有序集合的操作，那就是下面这样：
    - 添加一个成员对象；
    - 按照键值来删除一个成员对象；
    - 按照键值来查找一个成员对象；
    - 按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象；
    - 按照分值从小到大排序成员变量；
    
    如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与LRU缓存淘汰算法的解决方法类似。我们可以再按照键值构建一个散列表，这样按照key来删除、查找一个成员对象的时间复杂度就变成了O(1)。同时，借助跳表结构，其他操作也非常高效。实际上，Redis有序集合的操作还有另外一类，也就是查找成员对象的排名（Rank）或者根据排名区间查找成员对象。这个功能单纯用刚刚讲的这种组合结构就无法高效实现了。这块内容我后面的章节再讲。

### Java LinkedHashMap
LinkedHashMap也是通过散列表和链表组合在一起实现的。实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。LinkedHashMap是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。
    
>例子：
每次调用put()函数，往LinkedHashMap中添加数据的时候，都会将数据添加到链表的尾部，所以，在前四个操作完成之后，链表中的数据是下面这样：![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515212313.png)再次将键值为3的数据放入到LinkedHashMap的时候，会先查找这个键值是否已经有了，然后，再将已经存在的(3,11)删除，并且将新的(3,26)放到链表的尾部。所以，这个时候链表中的数据就是下面这样：![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515212301.png)代码访问到key为5的数据的时候，我们将被访问到的数据移动到链表的尾部。![](https://raw.githubusercontent.com/binbinbin5/myPics/master/imgs/20190515212340.png)


## 为什么散列表和链表经常一块使用？
散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。

## 代码

```
public SeparateChainingHashTable() {
        this(DEFAULT_TABLE_SIZE);
    }
public SeparateChainingHashTable(int size) {
    theLists=new LinkedList[nextPrime(size)];
    for(int i=0;i<theLists.length;i++) {
        theLists[i]=new LinkedList<>();//初始化链表数组
    }
}

/*
 * 哈希表插入元素
 * */
public void insert(T x) {
    List<T> whichList=theLists[myhash(x)];
    /*
     * 如果当前哈希地址的链表不含有元素，则链表中添加该元素
     * */
    if(!whichList.contains(x)) {
        whichList.add(x);
        if(++currentSize>theLists.length)//如果表长度不够，则扩容
            rehash();
    }
}
public void remove(T x) {
    List<T> whichList=theLists[myhash(x)];
    if(whichList.contains(x)) {
        whichList.remove(x);
        currentSize--;
    }
}
public boolean contains(T x) {
    List<T> whilchList=theLists[myhash(x)];
    return whilchList.contains(x);
}
public void makeEmpty() {
    for(int i=0;i<theLists.length;i++)
        theLists[i].clear();
    currentSize=0;
}


private void rehash() {
    List<T>[] oldLists=theLists;
    theLists=new List[nextPrime(2*theLists.length)];
    for(int j=0;j<theLists.length;j++)
        theLists[j]=new LinkedList<>();
    
    currentSize=0;
    /*
     * 更新哈希表
     * */
    for(List<T> list:oldLists)
        for(T item:list)
            insert(item);
}
/*
 * myhash()方法获得哈希表的地址
 * */
private int myhash(T x) {
    int hashVal=x.hashCode();//hashCode()方法返回该对象的哈希码值
    hashVal%=theLists.length;//对哈希表长度取余数
    if(hashVal<0)
        hashVal+=theLists.length;
    return hashVal;
}
```
